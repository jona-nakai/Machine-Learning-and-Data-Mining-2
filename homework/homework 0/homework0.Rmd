---
title: "homework0"
output: html_document
date: "2026-01-10"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r matrices}
x <- c(1, -2, 3)
y <- c(-4, 1)
W <- matrix(c(0, 1,
              2, -1,
              1, 3),
            nrow = 3, ncol=2, byrow=TRUE)
```

```{r 2(a)}
a <- t(W) %*% x - y
print(a)
```

```{r 2(b)}
b <- (y %*% t(y)) * (t(W) %*% W)
print(b)
```


```{r 2(c)}
c <- sqrt(sum(x^2)) - sum(abs(y))
print(c)
```

```{r define_functions}
# Defining function f(x) = 2x^4 - 3x^3 + 1
f <- function(x) {
  return(2*x^4 - 3*x^3 + 1)
}

# Defining function dfd(x) = x^2 * (8x - 9)
dfd <- function(x) {
  return(x^2 * (8*x - 9))
}

#If we take the derivative of our f(x) function, it comes out to
#8x^3 - 9x^2

#And if we factor our an x^2 from our derivative, we get_ipython
#x^2 * (8x - 9)

#This is our dfd(x) function. So dfd(x) is the derivative of f(x).
#This is useful for a gradient descent algorithm because the gradient descent
#algorithm requires the derivative of the function we are trying to find the
#minimum of our function.

# Our initial random value. Our hope is that by using GD,
# this will turn into the x value that minimizes our f(x).
x <- 0.5
# Our learning rate. We use this to slowly move towards
# our minimum.
eta <- 0.05
# Epochs defines how many times we apply the gradient descent
# formula to try and minimize our function.
epochs <- 20

# Keeping track of our f(x) for each x we find. This will help us
# visualize how our gradient descent algorithm slowly minimizes our
# function.
f_value_list <- numeric(epochs)
# Running our gradient descent algorithm for the amount of epochs we defined above.
for (i in 1:epochs) {
  # x plugged into our derivative function.
  dfdx <- dfd(x)
  # Applying gradient descent to move towards the x value that minimizes
    # our function.
  x <- x - eta*dfdx
  # Tracking our f(x) values after each epoch
  f_value_list[i] <- f(x)
}

# After performing gradient descent over 20 epochs, what x value did we find
# to minimize the function?
cat("Best x = \n", x)

# Plotting our f(x) values over 20 epochs to visualize how gradient descent
# minimizes our function.
plot(f_value_list, 
     type = "l", 
     col = "blue",
     main = "f(x) over GD",
     xlab = "steps",
     ylab = "f(x)")
```